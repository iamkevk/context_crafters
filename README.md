# Fine-tuning Google's Gemma (7b) LLM with Kaggle-documentation synthetic dataset 

## Introduction
This project is the work for the group `Context Crafters` in the Georgia Tech Deep Learning graduate class (Spring 2024). Please visit this page for details of the [CS 7643](https://omscs.gatech.edu/cs-7643-deep-learning) class.

## Project Overview

In February 2024, Google [released Gemma](https://blog.google/technology/developers/gemma-open-models/) Gemma, a family of lightweight open-source generative AI models, designed primarily for developers and researchers. Using Google's Gemma as the pre-trained model, we attempt in this project to fine-tune this foundation generative model to adapt it to perform better at question-answering content from [Kaggle documentation](https://www.kaggle.com/docs)  
